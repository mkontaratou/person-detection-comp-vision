{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person Detection and Tracking in Crowded Scenes using Classical Computer Vision Techniques\n",
    "\n",
    "Authors : Maria Kontaratou, Chaimae Sadoune & Manon Lagarde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prerequisites : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Import Statements : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imutils\n",
    "import xml.etree.ElementTree as ET\n",
    "from imutils.object_detection import non_max_suppression\n",
    "from scipy.spatial import distance as dist\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. File Path Configuration :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_image_path = '/Users/CentraleSupelec/Documents/ComputerVision_Project/person_img.jpg'\n",
    "xml_file = '/Users/CentraleSupelec/Documents/ComputerVision_Project/PETS09-S2L2/PETS2009-S2L2.xml'\n",
    "video_file = '/Users/CentraleSupelec/Documents/ComputerVision_Project/PETS09-S2L2/PETS09-S2L2-raw.webm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Pedestrian Detection using HOG and SVM : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by initializing a Histogram of Oriented Gradients (HOG) detector with a pre-trained Support Vector Machine (SVM) model for pedestrian detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Create the HOG descriptor and set the pre-trained SVM detector for people\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# 2) Load your test image \n",
    "img = cv2.imread(test_image_path)\n",
    "\n",
    "if img is None:\n",
    "    raise IOError(\"Could not read test image!\")\n",
    "\n",
    "# 3) Optionally resize for speed (comment out if you want original size)\n",
    "#    This step can help reduce false positives in large images\n",
    "original = img.copy()\n",
    "scale_percent = 100  \n",
    "width = int(img.shape[1] * scale_percent / 100)\n",
    "height = int(img.shape[0] * scale_percent / 100)\n",
    "img = cv2.resize(img, (width, height))\n",
    "\n",
    "# 4) Run the detector\n",
    "#    - winStride controls the step of the sliding window\n",
    "#    - padding is the area around the detection window\n",
    "#    - scale is how much the image is resized at each scale\n",
    "(rects, weights) = hog.detectMultiScale(\n",
    "    img,\n",
    "    winStride=(8, 8),\n",
    "    padding=(8, 8),\n",
    "    scale=1.1\n",
    ")\n",
    "\n",
    "# 5) Apply non-maximum suppression to reduce overlapping boxes\n",
    "#    OpenCV doesn’t automatically do NMS for HOG, so manually:\n",
    "rects_np = []\n",
    "for (x, y, w, h) in rects:\n",
    "    rects_np.append([x, y, x + w, y + h])  # x1, y1, x2, y2\n",
    "\n",
    "rects_np = np.array(rects_np)\n",
    "pick = non_max_suppression(rects_np, probs=None, overlapThresh=0.65)\n",
    "\n",
    "\n",
    "# 6) Visualize the results\n",
    "for (x1, y1, x2, y2) in pick:\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "\n",
    "# Convert to RGB (matplotlib uses RGB, OpenCV uses BGR)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(img_rgb)\n",
    "plt.title(f\"Detected Pedestrians: {len(pick)}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pedestrian Detection and Tracking using HOG and Centroid Tracking : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code detects and tracks people in a video using classical techniques:\n",
    "\n",
    "- HOG + SVM for detection.\n",
    "- Centroid-based tracking to maintain identities across frames.\n",
    "- Non-Maximum Suppression to reduce redundant detections.\n",
    "\n",
    "This approach is computationally lightweight and suitable for scenarios where deep learning isn't feasible. However, it has limitations in very crowded scenes and under occlusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is “bad” because classical HOG alone isn’t robust for small, occluded, or crowded scenes, and a simple centroid tracker easily confuses IDs without motion/appearance modeling. For a challenging dataset like PETS09-S2L1, you either need:\n",
    "\n",
    "- More advanced classical: (1) background subtraction + morphological filtering + (2) HOG + (3) multi-feature tracking (Kalman + color).\n",
    "- Modern deep methods: They handle scale/occlusion better but require training or at least a pre-trained YOLO/SSD-like network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Simple Centroid Tracker\n",
    "###########################################\n",
    "class CentroidTracker:\n",
    "    def __init__(self, maxDisappeared=30, maxDistance=50):\n",
    "        # Next available object ID\n",
    "        self.nextObjectID = 0\n",
    "        # Dictionary: objectID -> (centroid, bbox)\n",
    "        self.objects = {}\n",
    "        # Dictionary: objectID -> number of consecutive frames missing\n",
    "        self.disappeared = {}\n",
    "        self.maxDisappeared = maxDisappeared\n",
    "        self.maxDistance = maxDistance\n",
    "\n",
    "    def register(self, centroid, bbox):\n",
    "        self.objects[self.nextObjectID] = (centroid, bbox)\n",
    "        self.disappeared[self.nextObjectID] = 0\n",
    "        self.nextObjectID += 1\n",
    "\n",
    "    def deregister(self, objectID):\n",
    "        del self.objects[objectID]\n",
    "        del self.disappeared[objectID]\n",
    "\n",
    "    def update(self, rects):\n",
    "        # If no detections, mark existing objects as disappeared\n",
    "        if len(rects) == 0:\n",
    "            for objectID in list(self.disappeared.keys()):\n",
    "                self.disappeared[objectID] += 1\n",
    "                if self.disappeared[objectID] > self.maxDisappeared:\n",
    "                    self.deregister(objectID)\n",
    "            return self.objects\n",
    "\n",
    "        # Compute centroids for the new detections\n",
    "        inputCentroids = np.zeros((len(rects), 2), dtype=\"int\")\n",
    "        for (i, (x, y, w, h)) in enumerate(rects):\n",
    "            cX = int(x + w / 2)\n",
    "            cY = int(y + h / 2)\n",
    "            inputCentroids[i] = (cX, cY)\n",
    "\n",
    "        # If no objects are tracked, register all detections\n",
    "        if len(self.objects) == 0:\n",
    "            for i in range(0, len(inputCentroids)):\n",
    "                self.register(inputCentroids[i], rects[i])\n",
    "        else:\n",
    "            # Grab the set of object IDs and their centroids\n",
    "            objectIDs = list(self.objects.keys())\n",
    "            objectCentroids = np.array([self.objects[objID][0] for objID in objectIDs])\n",
    "\n",
    "            # Compute distance matrix between tracked centroids and new centroids\n",
    "            D = dist.cdist(objectCentroids, inputCentroids)\n",
    "            # For each existing object, find the closest new centroid\n",
    "            rows = D.min(axis=1).argsort()\n",
    "            cols = D.argmin(axis=1)[rows]\n",
    "\n",
    "            usedRows = set()\n",
    "            usedCols = set()\n",
    "\n",
    "            # Assign the new centroid to an object if the distance is within maxDistance\n",
    "            for (row, col) in zip(rows, cols):\n",
    "                if row in usedRows or col in usedCols:\n",
    "                    continue\n",
    "                if D[row, col] > self.maxDistance:\n",
    "                    continue\n",
    "                objectID = objectIDs[row]\n",
    "                self.objects[objectID] = (inputCentroids[col], rects[col])\n",
    "                self.disappeared[objectID] = 0\n",
    "                usedRows.add(row)\n",
    "                usedCols.add(col)\n",
    "\n",
    "            # Mark unmatched existing objects as disappeared\n",
    "            unusedRows = set(range(0, D.shape[0])).difference(usedRows)\n",
    "            for row in unusedRows:\n",
    "                objectID = objectIDs[row]\n",
    "                self.disappeared[objectID] += 1\n",
    "                if self.disappeared[objectID] > self.maxDisappeared:\n",
    "                    self.deregister(objectID)\n",
    "\n",
    "            # Register new detections that were not matched\n",
    "            unusedCols = set(range(0, D.shape[1])).difference(usedCols)\n",
    "            for col in unusedCols:\n",
    "                self.register(inputCentroids[col], rects[col])\n",
    "\n",
    "        return self.objects\n",
    "    \n",
    "###########################################\n",
    "# HOG Detector Function with NMS\n",
    "###########################################\n",
    "def detect_pedestrians(frame, conf_thresh=0.4):\n",
    "    \"\"\"\n",
    "    Detect pedestrians in the frame using OpenCV's HOG detector.\n",
    "    Returns a list of bounding boxes in (x, y, w, h) format.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale and optionally apply histogram equalization\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray_eq = cv2.equalizeHist(gray)\n",
    "    \n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    (rects, weights) = hog.detectMultiScale(\n",
    "        gray_eq,\n",
    "        winStride=(4, 4),\n",
    "        padding=(8, 8),\n",
    "        scale=1.05\n",
    "    )\n",
    "    \n",
    "    filtered_rects = []\n",
    "    for (r, w_val) in zip(rects, weights):\n",
    "        if w_val >= conf_thresh:\n",
    "            filtered_rects.append(r)\n",
    "    if len(filtered_rects) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Convert detections to (x1, y1, x2, y2) for non-maximum suppression\n",
    "    rects_np = np.array([[x, y, x + w, y + h] for (x, y, w, h) in filtered_rects])\n",
    "    picks = non_max_suppression(rects_np, probs=None, overlapThresh=0.65)\n",
    "    final_rects = [(x1, y1, x2 - x1, y2 - y1) for (x1, y1, x2, y2) in picks]\n",
    "    return final_rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 13:34:52.566 python[67375:2633284] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-15 13:34:52.566 python[67375:2633284] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# Main Processing Function\n",
    "###########################################\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    # Instantiate the centroid tracker\n",
    "    tracker = CentroidTracker(maxDisappeared=30, maxDistance=50)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Resize frame for consistency and processing speed\n",
    "        frame = imutils.resize(frame, width=800)\n",
    "        \n",
    "        # Detect pedestrians in the current frame\n",
    "        detections = detect_pedestrians(frame, conf_thresh=0.6)\n",
    "        \n",
    "        # Update the tracker with the current frame detections\n",
    "        objects = tracker.update(detections)\n",
    "        \n",
    "        # Draw detection bounding boxes (red) and tracked IDs (green)\n",
    "        for (x, y, w, h) in detections:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "        for objectID, (centroid, bbox) in objects.items():\n",
    "            (x, y, w, h) = bbox\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"ID {objectID}\", (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"PETS09-S2L1 - Detection and Tracking\", frame)\n",
    "        key = cv2.waitKey(30) & 0xFF\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Metrix Calculation - Pedestrian Detection and Performance Evaluation : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code evaluates a pedestrian detection system using HOG + SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# 1. Parse XML for Ground-Truth\n",
    "###########################################\n",
    "def parse_xml(xml_file):\n",
    "    \"\"\"\n",
    "    Parses an XML file with the structure:\n",
    "      <dataset>\n",
    "         <frame number=\"0\">\n",
    "            <objectlist>\n",
    "               <object id=\"9\">\n",
    "                  <box h=\"75.17\" w=\"31.03\" xc=\"514.7109\" yc=\"195.2731\"/>\n",
    "               </object>\n",
    "               ...\n",
    "            </objectlist>\n",
    "         </frame>\n",
    "         <frame number=\"1\"> ... </frame>\n",
    "         ...\n",
    "      </dataset>\n",
    "\n",
    "    Returns a dictionary:\n",
    "    ground_truth[frame_number] = [ (x, y, w, h), ... ]\n",
    "    We ignore the 'id' attribute for detection metrics.\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    ground_truth = {}\n",
    "    for frame_elem in root.findall('frame'):\n",
    "        frame_num = int(frame_elem.get('number'))\n",
    "        objectlist = frame_elem.find('objectlist')\n",
    "        boxes = []\n",
    "        if objectlist is not None:\n",
    "            for obj in objectlist.findall('object'):\n",
    "                box_elem = obj.find('box')\n",
    "                if box_elem is not None:\n",
    "                    w = float(box_elem.get('w'))\n",
    "                    h = float(box_elem.get('h'))\n",
    "                    xc = float(box_elem.get('xc'))\n",
    "                    yc = float(box_elem.get('yc'))\n",
    "                    # Convert center coords to top-left\n",
    "                    x = xc - w/2.0\n",
    "                    y = yc - h/2.0\n",
    "                    boxes.append((x, y, w, h))\n",
    "        ground_truth[frame_num] = boxes\n",
    "    return ground_truth\n",
    "\n",
    "###########################################\n",
    "# 2. IoU and Matching Functions\n",
    "###########################################\n",
    "def compute_iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "    Computes Intersection over Union (IoU) between two boxes in (x, y, w, h) format.\n",
    "    \"\"\"\n",
    "    (xA, yA, wA, hA) = boxA\n",
    "    (xB, yB, wB, hB) = boxB\n",
    "    xA2, yA2 = xA + wA, yA + hA\n",
    "    xB2, yB2 = xB + wB, yB + hB\n",
    "\n",
    "    interX1 = max(xA, xB)\n",
    "    interY1 = max(yA, yB)\n",
    "    interX2 = min(xA2, xB2)\n",
    "    interY2 = min(yA2, yB2)\n",
    "    interW = max(0, interX2 - interX1)\n",
    "    interH = max(0, interY2 - interY1)\n",
    "    intersection = interW * interH\n",
    "\n",
    "    areaA = wA * hA\n",
    "    areaB = wB * hB\n",
    "    union = areaA + areaB - intersection\n",
    "    if union <= 0:\n",
    "        return 0.0\n",
    "    return intersection / float(union)\n",
    "\n",
    "def match_boxes(pred_boxes, gt_boxes, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Matches predicted boxes to ground-truth boxes using IoU.\n",
    "    Returns:\n",
    "      - tp_matches: list of tuples (pred_idx, gt_idx, iou_val)\n",
    "      - fp_idxs: list of prediction indices not matched\n",
    "      - fn_idxs: list of GT indices not matched\n",
    "    \"\"\"\n",
    "    tp_matches = []\n",
    "    used_gt = set()\n",
    "    used_pred = set()\n",
    "\n",
    "    for pred_idx, pbox in enumerate(pred_boxes):\n",
    "        best_iou = 0\n",
    "        best_gt_idx = None\n",
    "        for gt_idx, gbox in enumerate(gt_boxes):\n",
    "            if gt_idx in used_gt:\n",
    "                continue\n",
    "            iou_val = compute_iou(pbox, gbox)\n",
    "            if iou_val > best_iou:\n",
    "                best_iou = iou_val\n",
    "                best_gt_idx = gt_idx\n",
    "        if best_iou >= iou_threshold and best_gt_idx is not None:\n",
    "            tp_matches.append((pred_idx, best_gt_idx, best_iou))\n",
    "            used_gt.add(best_gt_idx)\n",
    "            used_pred.add(pred_idx)\n",
    "\n",
    "    fp_idxs = [i for i in range(len(pred_boxes)) if i not in used_pred]\n",
    "    fn_idxs = [j for j in range(len(gt_boxes)) if j not in used_gt]\n",
    "    return tp_matches, fp_idxs, fn_idxs\n",
    "\n",
    "###########################################\n",
    "# 3. HOG Detector Function with NMS\n",
    "###########################################\n",
    "def detect_pedestrians(frame, conf_thresh=0.4):\n",
    "    \"\"\"\n",
    "    Runs HOG detection on a single frame.\n",
    "    Returns bounding boxes in (x, y, w, h).\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray_eq = cv2.equalizeHist(gray)\n",
    "\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    (rects, weights) = hog.detectMultiScale(\n",
    "        gray_eq,\n",
    "        winStride=(4,4),\n",
    "        padding=(8,8),\n",
    "        scale=1.05\n",
    "    )\n",
    "    # Filter by confidence threshold\n",
    "    filtered = []\n",
    "    for (r, w_val) in zip(rects, weights):\n",
    "        if w_val >= conf_thresh:\n",
    "            filtered.append(r)\n",
    "\n",
    "    if len(filtered) == 0:\n",
    "        return []\n",
    "\n",
    "    # NMS\n",
    "    rects_np = np.array([[x, y, x+w, y+h] for (x,y,w,h) in filtered])\n",
    "    picks = non_max_suppression(rects_np, probs=None, overlapThresh=0.65)\n",
    "    boxes = [(x1, y1, (x2 - x1), (y2 - y1)) for (x1,y1,x2,y2) in picks]\n",
    "    return boxes\n",
    "\n",
    "###########################################\n",
    "# 4. Evaluate Detector Over Video\n",
    "###########################################\n",
    "def evaluate_detector(ground_truth, video_path, detector, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    :param ground_truth: dict { frame_idx: [ (x,y,w,h), ... ] }\n",
    "    :param video_path: path to the video\n",
    "    :param detector: function that returns predicted boxes\n",
    "    :param iou_threshold: IoU threshold for matching\n",
    "    Returns detection metrics: (precision, recall, f1, avg_iou, dice)\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_idx = 0\n",
    "\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "    iou_sum = 0\n",
    "    match_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # If this frame_idx not in ground_truth, skip\n",
    "        # or treat it as empty ground-truth\n",
    "        gt_boxes = ground_truth.get(frame_idx, [])\n",
    "\n",
    "        # Optional: resize for consistent processing\n",
    "        frame = imutils.resize(frame, width=800)\n",
    "\n",
    "        # Run the detector\n",
    "        pred_boxes = detector(frame)\n",
    "\n",
    "        # Match predictions to ground-truth\n",
    "        tp_matches, fp_idxs, fn_idxs = match_boxes(pred_boxes, gt_boxes, iou_threshold)\n",
    "        total_tp += len(tp_matches)\n",
    "        total_fp += len(fp_idxs)\n",
    "        total_fn += len(fn_idxs)\n",
    "\n",
    "        for (_, _, iou_val) in tp_matches:\n",
    "            iou_sum += iou_val\n",
    "            match_count += 1\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    precision = total_tp / float(total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
    "    recall = total_tp / float(total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
    "    f1 = 2.0 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    avg_iou = iou_sum / match_count if match_count > 0 else 0.0\n",
    "    dice = f1  # For bounding-box detection, F1 is typically the same as Dice\n",
    "\n",
    "    return precision, recall, f1, avg_iou, dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DETECTION METRICS ===\n",
      "Precision: 0.0013\n",
      "Recall:    0.0004\n",
      "F1 Score:  0.0006\n",
      "Avg IoU:   0.5226\n",
      "Dice:      0.0006\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# 5. Main\n",
    "###########################################\n",
    "def main():\n",
    "    # 1) Parse XML\n",
    "    ground_truth = parse_xml(xml_file)\n",
    "\n",
    "    # 2) Evaluate HOG detection\n",
    "    precision, recall, f1, avg_iou, dice = evaluate_detector(\n",
    "        ground_truth,\n",
    "        video_file,\n",
    "        detector=lambda frm: detect_pedestrians(frm, conf_thresh=0.4),\n",
    "        iou_threshold=0.5\n",
    "    )\n",
    "\n",
    "    print(\"=== DETECTION METRICS ===\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"Avg IoU:   {avg_iou:.4f}\")\n",
    "    print(f\"Dice:      {dice:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hybrid Pedestrian Tracking Pipeline : Background Subtraction, HOG Detection, and Kalman-Hungarian Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This hybrid pedestrian tracking pipeline enhances detection and tracking accuracy by combining multiple classical computer vision techniques:\n",
    "\n",
    "- Background Subtraction (MOG2): Isolates moving objects by removing static background elements.\n",
    "- HOG Detection: Identifies pedestrians using a pre-trained SVM classifier.\n",
    "- Fusion of Detections: Merges background subtraction and HOG results for improved accuracy.\n",
    "- Tracking with Kalman Filter: Predicts object positions to handle occlusions and maintain identity.\n",
    "- Hungarian Algorithm for Assignment: Matches detections to tracked objects based on motion and appearance.\n",
    "- Appearance-Based Matching (Color Histograms): Reduces ID switches by incorporating object color similarity.\n",
    "- Non-Maximum Suppression (NMS): Eliminates redundant overlapping detections for cleaner results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 1. Helper: Simple Color Histogram Extraction\n",
    "##############################################\n",
    "def extract_color_hist(frame, bbox, histSize=8):\n",
    "    \"\"\"\n",
    "    Extract a simple HSV color histogram from 'bbox' in 'frame'.\n",
    "    bbox = (x, y, w, h).\n",
    "    Returns a 1D normalized histogram of length histSize*3.\n",
    "    \"\"\"\n",
    "    (x, y, w, h) = bbox\n",
    "    frame_h, frame_w = frame.shape[:2]\n",
    "    x = max(0, min(x, frame_w - 1))\n",
    "    y = max(0, min(y, frame_h - 1))\n",
    "    w = max(0, min(w, frame_w - x))\n",
    "    h = max(0, min(h, frame_h - y))\n",
    "    roi = frame[y:y+h, x:x+w]\n",
    "    if roi.size == 0:\n",
    "        return np.zeros((histSize*3,), dtype=np.float32)\n",
    "\n",
    "    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "    h_hist = cv2.calcHist([hsv], [0], None, [histSize], [0, 180])\n",
    "    s_hist = cv2.calcHist([hsv], [1], None, [histSize], [0, 256])\n",
    "    v_hist = cv2.calcHist([hsv], [2], None, [histSize], [0, 256])\n",
    "\n",
    "    h_hist = cv2.normalize(h_hist, h_hist, 0, 1, cv2.NORM_MINMAX).flatten()\n",
    "    s_hist = cv2.normalize(s_hist, s_hist, 0, 1, cv2.NORM_MINMAX).flatten()\n",
    "    v_hist = cv2.normalize(v_hist, v_hist, 0, 1, cv2.NORM_MINMAX).flatten()\n",
    "    return np.concatenate([h_hist, s_hist, v_hist])\n",
    "\n",
    "def bhattacharyya_distance(histA, histB):\n",
    "    \"\"\"\n",
    "    Bhattacharyya distance between two normalized histograms.\n",
    "    Lower = more similar.\n",
    "    \"\"\"\n",
    "    overlap = np.sum(np.sqrt(histA * histB))\n",
    "    overlap = max(0.0, min(overlap, 1.0))\n",
    "    return np.sqrt(1.0 - overlap)\n",
    "\n",
    "##############################################\n",
    "# 2. Kalman Filter Helpers\n",
    "##############################################\n",
    "def create_kalman_filter():\n",
    "    kf = cv2.KalmanFilter(4, 2)\n",
    "    kf.transitionMatrix = np.array([[1, 0, 1, 0],\n",
    "                                    [0, 1, 0, 1],\n",
    "                                    [0, 0, 1, 0],\n",
    "                                    [0, 0, 0, 1]], np.float32)\n",
    "    kf.measurementMatrix = np.eye(2, 4, dtype=np.float32)\n",
    "    kf.processNoiseCov = np.eye(4, 4, dtype=np.float32) * 0.03\n",
    "    kf.measurementNoiseCov = np.eye(2, 2, dtype=np.float32) * 0.5\n",
    "    kf.errorCovPost = np.eye(4, 4, dtype=np.float32)\n",
    "    return kf\n",
    "\n",
    "def kalman_predict_centroid(kf):\n",
    "    pred = kf.predict()\n",
    "    return (pred[0,0], pred[1,0])\n",
    "\n",
    "def kalman_correct_centroid(kf, x, y):\n",
    "    measurement = np.array([[np.float32(x)], [np.float32(y)]])\n",
    "    kf.correct(measurement)\n",
    "\n",
    "##############################################\n",
    "# 3. Multi-Object Tracker: Kalman + Hungarian\n",
    "#    with optional appearance cost\n",
    "##############################################\n",
    "class MultiObjectTracker:\n",
    "    def __init__(self, max_disappeared=10, max_distance=30, alpha=0.8):\n",
    "        \"\"\"\n",
    "        :param max_disappeared: frames to allow a track to vanish\n",
    "        :param max_distance: max centroid distance for matching\n",
    "        :param alpha: weighting factor for distance vs. color cost\n",
    "                      cost = alpha*distance + (1-alpha)*colorDist\n",
    "                      (1-alpha)=0 => purely distance-based\n",
    "        \"\"\"\n",
    "        self.nextID = 0\n",
    "        self.tracks = {}  # objectID -> { 'kf':..., 'bbox':..., 'hist':..., 'disappeared':... }\n",
    "        self.max_disappeared = max_disappeared\n",
    "        self.max_distance = max_distance\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def register(self, centroid, bbox, hist):\n",
    "        kf = create_kalman_filter()\n",
    "        kalman_correct_centroid(kf, centroid[0], centroid[1])\n",
    "        self.tracks[self.nextID] = {\n",
    "            'kf': kf,\n",
    "            'bbox': bbox,\n",
    "            'hist': hist,\n",
    "            'disappeared': 0\n",
    "        }\n",
    "        self.nextID += 1\n",
    "\n",
    "    def deregister(self, objectID):\n",
    "        del self.tracks[objectID]\n",
    "\n",
    "    def predict_all(self):\n",
    "        preds = {}\n",
    "        for objID, data in self.tracks.items():\n",
    "            (px, py) = kalman_predict_centroid(data['kf'])\n",
    "            preds[objID] = (px, py)\n",
    "        return preds\n",
    "\n",
    "    def update(self, frame, detections):\n",
    "        \"\"\"\n",
    "        :param frame: current frame for color hist extraction\n",
    "        :param detections: list of (x, y, w, h) from combined detection\n",
    "        \"\"\"\n",
    "        if len(detections) == 0:\n",
    "            # Mark all tracks as disappeared\n",
    "            for objID in list(self.tracks.keys()):\n",
    "                self.tracks[objID]['disappeared'] += 1\n",
    "                if self.tracks[objID]['disappeared'] > self.max_disappeared:\n",
    "                    self.deregister(objID)\n",
    "            return self.tracks\n",
    "\n",
    "        # Compute centroids + color hist for each detection\n",
    "        input_data = []\n",
    "        for (x, y, w, h) in detections:\n",
    "            cX = x + w/2.0\n",
    "            cY = y + h/2.0\n",
    "            hist = extract_color_hist(frame, (x, y, w, h), histSize=8)\n",
    "            input_data.append((cX, cY, (x,y,w,h), hist))\n",
    "\n",
    "        if len(self.tracks) == 0:\n",
    "            # Register all\n",
    "            for (cX, cY, box, hist) in input_data:\n",
    "                self.register((cX, cY), box, hist)\n",
    "        else:\n",
    "            preds = self.predict_all()\n",
    "            objIDs = list(self.tracks.keys())\n",
    "            objCentroids = np.array([preds[id] for id in objIDs])\n",
    "\n",
    "            # Build cost matrix (#tracks x #detections)\n",
    "            cost_matrix = np.zeros((len(objIDs), len(input_data)), dtype=np.float32)\n",
    "\n",
    "            for i, objID in enumerate(objIDs):\n",
    "                (px, py) = objCentroids[i]\n",
    "                objHist = self.tracks[objID]['hist']\n",
    "\n",
    "                for j, (cX, cY, box, detHist) in enumerate(input_data):\n",
    "                    dist_val = dist.euclidean((px, py), (cX, cY))\n",
    "                    # Normalize distance to [0..1] w.r.t max_distance\n",
    "                    norm_dist = dist_val / float(self.max_distance)\n",
    "                    if norm_dist > 1.0:\n",
    "                        norm_dist = 1.0\n",
    "                    colorDist = bhattacharyya_distance(objHist, detHist)\n",
    "                    # Weighted cost\n",
    "                    costVal = self.alpha * norm_dist + (1.0 - self.alpha) * colorDist\n",
    "                    cost_matrix[i, j] = costVal\n",
    "\n",
    "            rows, cols = linear_sum_assignment(cost_matrix)\n",
    "            used_rows = set()\n",
    "            used_cols = set()\n",
    "\n",
    "            for (row, col) in zip(rows, cols):\n",
    "                costVal = cost_matrix[row, col]\n",
    "                if costVal > 1.0:\n",
    "                    # If costVal > 1, treat it as no match\n",
    "                    continue\n",
    "                objID = objIDs[row]\n",
    "                (cX, cY, box, detHist) = input_data[col]\n",
    "                (x, y, w, h) = box\n",
    "\n",
    "                # Correct Kalman\n",
    "                kalman_correct_centroid(self.tracks[objID]['kf'], cX, cY)\n",
    "                self.tracks[objID]['bbox'] = (x, y, w, h)\n",
    "                self.tracks[objID]['hist'] = detHist\n",
    "                self.tracks[objID]['disappeared'] = 0\n",
    "                used_rows.add(row)\n",
    "                used_cols.add(col)\n",
    "\n",
    "            # Unmatched existing tracks => disappeared++\n",
    "            unused_rows = set(range(cost_matrix.shape[0])) - used_rows\n",
    "            for row in unused_rows:\n",
    "                objID = objIDs[row]\n",
    "                self.tracks[objID]['disappeared'] += 1\n",
    "                if self.tracks[objID]['disappeared'] > self.max_disappeared:\n",
    "                    self.deregister(objID)\n",
    "\n",
    "            # Unmatched detections => register new\n",
    "            unused_cols = set(range(cost_matrix.shape[1])) - used_cols\n",
    "            for col in unused_cols:\n",
    "                (cX, cY, box, detHist) = input_data[col]\n",
    "                self.register((cX, cY), box, detHist)\n",
    "\n",
    "        return self.tracks\n",
    "\n",
    "##############################################\n",
    "# 4. Combined Detection: Background Sub + HOG\n",
    "##############################################\n",
    "def background_subtraction(frame, fgbg,\n",
    "                           min_area=1000, max_area=20000,\n",
    "                           min_ratio=0.3, max_ratio=3.0):\n",
    "    \"\"\"\n",
    "    Returns bounding boxes from background subtraction + morphology.\n",
    "    \"\"\"\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7,7))\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "    contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    boxes = []\n",
    "    for c in contours:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        area = w * h\n",
    "        if area < min_area or area > max_area:\n",
    "            continue\n",
    "        ratio = w / float(h)\n",
    "        if ratio < min_ratio or ratio > max_ratio:\n",
    "            continue\n",
    "        boxes.append((x, y, w, h))\n",
    "    return boxes\n",
    "\n",
    "def hog_detect(frame, conf_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Returns bounding boxes from HOG detection (with eqHist).\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray_eq = cv2.equalizeHist(gray)\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    (rects, weights) = hog.detectMultiScale(\n",
    "        gray_eq,\n",
    "        winStride=(4, 4),\n",
    "        padding=(8, 8),\n",
    "        scale=1.05\n",
    "    )\n",
    "    # Filter by confidence\n",
    "    filtered = []\n",
    "    for (r, w_val) in zip(rects, weights):\n",
    "        if w_val >= conf_thresh:\n",
    "            filtered.append(r)\n",
    "    if len(filtered) == 0:\n",
    "        return []\n",
    "    # NMS\n",
    "    rects_np = np.array([[x, y, x+w, y+h] for (x,y,w,h) in filtered])\n",
    "    picks = non_max_suppression(rects_np, probs=None, overlapThresh=0.65)\n",
    "    boxes = [(x1, y1, x2-x1, y2-y1) for (x1,y1,x2,y2) in picks]\n",
    "    return boxes\n",
    "\n",
    "def combine_detections(bg_boxes, hog_boxes, iou_thresh=0.3):\n",
    "    \"\"\"\n",
    "    Merge background-subtractor boxes and HOG boxes.\n",
    "    If a HOG box overlaps a BG box with IoU> iou_thresh, treat them as one.\n",
    "    Otherwise, keep both.\n",
    "    \"\"\"\n",
    "    all_boxes = bg_boxes + hog_boxes\n",
    "    # Convert to x1,y1,x2,y2\n",
    "    box_xyxy = []\n",
    "    for (x,y,w,h) in all_boxes:\n",
    "        box_xyxy.append([x, y, x+w, y+h])\n",
    "\n",
    "    # Then do NMS with a lower threshold to unify overlapping boxes\n",
    "    merged = non_max_suppression(np.array(box_xyxy), overlapThresh=iou_thresh)\n",
    "    final = []\n",
    "    for (x1,y1,x2,y2) in merged:\n",
    "        final.append((x1, y1, x2-x1, y2-y1))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/78/48cyn65d1b1byv818mmyxcx40000gp/T/ipykernel_67375/703449523.py:35: RuntimeWarning: invalid value encountered in sqrt\n",
      "  overlap = np.sum(np.sqrt(histA * histB))\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# 5. Main\n",
    "##############################################\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    # Background subtractor\n",
    "    fgbg = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=40, detectShadows=False)\n",
    "\n",
    "    # Multi-object tracker with appearance cost\n",
    "    # alpha=0.8 => mostly distance-based, 0.2 => color-based\n",
    "    tracker = MultiObjectTracker(max_disappeared=10, max_distance=30, alpha=0.8)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = imutils.resize(frame, width=800)\n",
    "\n",
    "        # 1) Get BG-sub boxes\n",
    "        bg_boxes = background_subtraction(frame, fgbg, min_area=1000, max_area=20000)\n",
    "        # 2) Get HOG boxes\n",
    "        hog_boxes = hog_detect(frame, conf_thresh=0.5)\n",
    "        # 3) Combine them (some may overlap)\n",
    "        combined_boxes = combine_detections(bg_boxes, hog_boxes, iou_thresh=0.3)\n",
    "\n",
    "        # 4) Update tracker\n",
    "        tracks = tracker.update(frame, combined_boxes)\n",
    "\n",
    "        # 5) Visualization\n",
    "        # Draw BG boxes in red, HOG boxes in blue, final tracked in green\n",
    "        for (x,y,w,h) in bg_boxes:\n",
    "            cv2.rectangle(frame, (x,y), (x+w,y+h), (0,0,255), 2)\n",
    "        for (x,y,w,h) in hog_boxes:\n",
    "            cv2.rectangle(frame, (x,y), (x+w,y+h), (255,0,0), 2)\n",
    "\n",
    "        for objID, data in tracks.items():\n",
    "            (bx, by, bw, bh) = data['bbox']\n",
    "            cv2.rectangle(frame, (bx, by), (bx+bw, by+bh), (0,255,0), 2)\n",
    "            cv2.putText(frame, f\"ID {objID}\", (bx, by - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "\n",
    "        cv2.imshow(\"PETS09 - Combined Detection + Tracking\", frame)\n",
    "        key = cv2.waitKey(30) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Metrix Calculation 2 : Enhanced Performance Evaluation with Appearance-Based Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section extends the initial performance evaluation by integrating appearance-based matching into the tracking process. It improves pedestrian tracking by combining:\n",
    "\n",
    "- Color Histogram Matching (HSV + Bhattacharyya Distance): Helps re-identify objects across frames.\n",
    "- Motion Prediction (Kalman Filter): Handles object occlusions and missing detections.\n",
    "- Detection Fusion (Background Subtraction + HOG): Improves robustness by merging multiple detection sources.\n",
    "- Assignment Optimization (Hungarian Algorithm): Matches detections to tracked objects using both motion and appearance features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# 4. Color Histogram Extraction and Bhattacharyya\n",
    "###########################################\n",
    "def extract_color_hist(frame, bbox, histSize=8):\n",
    "    \"\"\"\n",
    "    Extracts a simple HSV color histogram from the given bounding box in 'frame'.\n",
    "    bbox = (x, y, w, h).\n",
    "    Returns a normalized 1D histogram.\n",
    "    \"\"\"\n",
    "    (x, y, w, h) = bbox\n",
    "    frame_h, frame_w = frame.shape[:2]\n",
    "    x = max(0, min(x, frame_w - 1))\n",
    "    y = max(0, min(y, frame_h - 1))\n",
    "    w = max(0, min(w, frame_w - x))\n",
    "    h = max(0, min(h, frame_h - y))\n",
    "    roi = frame[y:y+h, x:x+w]\n",
    "    if roi.size == 0:\n",
    "        return np.zeros((histSize*3,), dtype=np.float32)\n",
    "    \n",
    "    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "    h_hist = cv2.calcHist([hsv], [0], None, [histSize], [0,180])\n",
    "    s_hist = cv2.calcHist([hsv], [1], None, [histSize], [0,256])\n",
    "    v_hist = cv2.calcHist([hsv], [2], None, [histSize], [0,256])\n",
    "    \n",
    "    h_hist = cv2.normalize(h_hist, h_hist, 0, 1, cv2.NORM_MINMAX).flatten()\n",
    "    s_hist = cv2.normalize(s_hist, s_hist, 0, 1, cv2.NORM_MINMAX).flatten()\n",
    "    v_hist = cv2.normalize(v_hist, v_hist, 0, 1, cv2.NORM_MINMAX).flatten()\n",
    "    return np.concatenate([h_hist, s_hist, v_hist])\n",
    "\n",
    "def bhattacharyya_distance(histA, histB):\n",
    "    overlap = np.sum(np.sqrt(histA * histB))\n",
    "    overlap = max(0.0, min(overlap, 1.0))\n",
    "    return np.sqrt(1.0 - overlap)\n",
    "\n",
    "###########################################\n",
    "# 5. Multi-Object Tracker: Kalman Filter + Hungarian Assignment\n",
    "###########################################\n",
    "class MultiObjectTracker:\n",
    "    def __init__(self, max_disappeared=10, max_distance=30, alpha=0.8):\n",
    "        \"\"\"\n",
    "        :param max_disappeared: frames allowed to miss before removal.\n",
    "        :param max_distance: maximum allowed centroid distance for matching.\n",
    "        :param alpha: weighting factor for combining normalized distance and color cost.\n",
    "                      (cost = alpha*(distance/max_distance) + (1-alpha)*colorDist)\n",
    "        \"\"\"\n",
    "        self.nextID = 0\n",
    "        self.tracks = {}  # objectID -> { 'kf': ..., 'bbox': ..., 'hist': ..., 'disappeared': ... }\n",
    "        self.max_disappeared = max_disappeared\n",
    "        self.max_distance = max_distance\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def register(self, centroid, bbox, hist):\n",
    "        kf = create_kalman_filter()\n",
    "        kalman_correct_centroid(kf, centroid[0], centroid[1])\n",
    "        self.tracks[self.nextID] = {\n",
    "            'kf': kf,\n",
    "            'bbox': bbox,\n",
    "            'hist': hist,\n",
    "            'disappeared': 0\n",
    "        }\n",
    "        self.nextID += 1\n",
    "\n",
    "    def deregister(self, objectID):\n",
    "        del self.tracks[objectID]\n",
    "\n",
    "    def predict_all(self):\n",
    "        preds = {}\n",
    "        for objID, data in self.tracks.items():\n",
    "            preds[objID] = kalman_predict_centroid(data['kf'])\n",
    "        return preds\n",
    "\n",
    "    def update(self, frame, detections):\n",
    "        \"\"\"\n",
    "        :param frame: current frame (for appearance extraction)\n",
    "        :param detections: list of bounding boxes (x,y,w,h)\n",
    "        :return: updated tracks dictionary\n",
    "        \"\"\"\n",
    "        if len(detections) == 0:\n",
    "            for objID in list(self.tracks.keys()):\n",
    "                self.tracks[objID]['disappeared'] += 1\n",
    "                if self.tracks[objID]['disappeared'] > self.max_disappeared:\n",
    "                    self.deregister(objID)\n",
    "            return self.tracks\n",
    "\n",
    "        input_data = []\n",
    "        for (x, y, w, h) in detections:\n",
    "            cX = x + w/2.0\n",
    "            cY = y + h/2.0\n",
    "            hist = extract_color_hist(frame, (x, y, w, h), histSize=8)\n",
    "            input_data.append((cX, cY, (x, y, w, h), hist))\n",
    "\n",
    "        if len(self.tracks) == 0:\n",
    "            for (cX, cY, box, hist) in input_data:\n",
    "                self.register((cX, cY), box, hist)\n",
    "        else:\n",
    "            preds = self.predict_all()\n",
    "            objIDs = list(self.tracks.keys())\n",
    "            objCentroids = np.array([preds[objID] for objID in objIDs])\n",
    "            cost_matrix = np.zeros((len(objIDs), len(input_data)), dtype=np.float32)\n",
    "\n",
    "            for i, objID in enumerate(objIDs):\n",
    "                (px, py) = objCentroids[i]\n",
    "                objHist = self.tracks[objID]['hist']\n",
    "                for j, (cX, cY, box, detHist) in enumerate(input_data):\n",
    "                    d = dist.euclidean((px, py), (cX, cY))\n",
    "                    norm_d = d / float(self.max_distance)\n",
    "                    if norm_d > 1.0:\n",
    "                        norm_d = 1.0\n",
    "                    color_cost = bhattacharyya_distance(objHist, detHist)\n",
    "                    cost = self.alpha * norm_d + (1.0 - self.alpha) * color_cost\n",
    "                    cost_matrix[i, j] = cost\n",
    "\n",
    "            rows, cols = linear_sum_assignment(cost_matrix)\n",
    "            used_rows = set()\n",
    "            used_cols = set()\n",
    "            for (row, col) in zip(rows, cols):\n",
    "                if cost_matrix[row, col] > 1.0:\n",
    "                    continue\n",
    "                objID = objIDs[row]\n",
    "                (cX, cY, box, detHist) = input_data[col]\n",
    "                (x, y, w, h) = box\n",
    "                kalman_correct_centroid(self.tracks[objID]['kf'], cX, cY)\n",
    "                self.tracks[objID]['bbox'] = box\n",
    "                self.tracks[objID]['hist'] = detHist\n",
    "                self.tracks[objID]['disappeared'] = 0\n",
    "                used_rows.add(row)\n",
    "                used_cols.add(col)\n",
    "\n",
    "            unused_rows = set(range(cost_matrix.shape[0])) - used_rows\n",
    "            for row in unused_rows:\n",
    "                objID = objIDs[row]\n",
    "                self.tracks[objID]['disappeared'] += 1\n",
    "                if self.tracks[objID]['disappeared'] > self.max_disappeared:\n",
    "                    self.deregister(objID)\n",
    "\n",
    "            unused_cols = set(range(cost_matrix.shape[1])) - used_cols\n",
    "            for col in unused_cols:\n",
    "                (cX, cY, box, detHist) = input_data[col]\n",
    "                self.register((cX, cY), box, detHist)\n",
    "\n",
    "        return self.tracks\n",
    "\n",
    "###########################################\n",
    "# 6. Hybrid Detection: Combine BG Sub and HOG\n",
    "###########################################\n",
    "def background_subtraction(frame, fgbg, min_area=1000, max_area=20000, min_ratio=0.3, max_ratio=3.0):\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7,7))\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "    contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    boxes = []\n",
    "    for c in contours:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        area = w * h\n",
    "        if area < min_area or area > max_area:\n",
    "            continue\n",
    "        ratio = w / float(h)\n",
    "        if ratio < min_ratio or ratio > max_ratio:\n",
    "            continue\n",
    "        boxes.append((x, y, w, h))\n",
    "    return boxes\n",
    "\n",
    "def hog_detect(frame, conf_thresh=0.5):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray_eq = cv2.equalizeHist(gray)\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    (rects, weights) = hog.detectMultiScale(\n",
    "        gray_eq,\n",
    "        winStride=(4,4),\n",
    "        padding=(8,8),\n",
    "        scale=1.05\n",
    "    )\n",
    "    filtered = []\n",
    "    for (r, w_val) in zip(rects, weights):\n",
    "        if w_val >= conf_thresh:\n",
    "            filtered.append(r)\n",
    "    if len(filtered) == 0:\n",
    "        return []\n",
    "    rects_np = np.array([[x, y, x+w, y+h] for (x,y,w,h) in filtered])\n",
    "    picks = non_max_suppression(rects_np, probs=None, overlapThresh=0.65)\n",
    "    boxes = [(x1, y1, x2-x1, y2-y1) for (x1,y1,x2,y2) in picks]\n",
    "    return boxes\n",
    "\n",
    "def combine_detections(bg_boxes, hog_boxes, iou_thresh=0.3):\n",
    "    all_boxes = bg_boxes + hog_boxes\n",
    "    box_xyxy = []\n",
    "    for (x, y, w, h) in all_boxes:\n",
    "        box_xyxy.append([x, y, x+w, y+h])\n",
    "    merged = non_max_suppression(np.array(box_xyxy), overlapThresh=iou_thresh)\n",
    "    final = [(x1, y1, x2-x1, y2-y1) for (x1, y1, x2, y2) in merged]\n",
    "    return final\n",
    "\n",
    "def hybrid_detections(frame, fgbg, conf_thresh=0.5):\n",
    "    bg_boxes = background_subtraction(frame, fgbg, min_area=1000, max_area=20000)\n",
    "    hog_boxes = hog_detect(frame, conf_thresh=conf_thresh)\n",
    "    combined = combine_detections(bg_boxes, hog_boxes, iou_thresh=0.3)\n",
    "    return combined\n",
    "\n",
    "###########################################\n",
    "# 7. Evaluation for Hybrid Model\n",
    "###########################################\n",
    "def evaluate_hybrid(ground_truth, video_path, fgbg, tracker, iou_threshold=0.5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_idx = 0\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "    iou_sum = 0\n",
    "    match_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = imutils.resize(frame, width=800)\n",
    "        gt_boxes = ground_truth.get(frame_idx, [])\n",
    "        combined_boxes = hybrid_detections(frame, fgbg, conf_thresh=0.5)\n",
    "        tracks = tracker.update(frame, combined_boxes)\n",
    "        pred_boxes = [data['bbox'] for data in tracks.values()]\n",
    "\n",
    "        tp, fp, fn = match_boxes(pred_boxes, gt_boxes, iou_threshold)\n",
    "        total_tp += len(tp)\n",
    "        total_fp += len(fp)\n",
    "        total_fn += len(fn)\n",
    "        for (_, _, iou_val) in tp:\n",
    "            iou_sum += iou_val\n",
    "            match_count += 1\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    precision = total_tp / float(total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / float(total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    avg_iou = iou_sum / match_count if match_count > 0 else 0\n",
    "    dice = f1\n",
    "    return precision, recall, f1, avg_iou, dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/78/48cyn65d1b1byv818mmyxcx40000gp/T/ipykernel_67375/1527674668.py:31: RuntimeWarning: invalid value encountered in sqrt\n",
      "  overlap = np.sum(np.sqrt(histA * histB))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HYBRID MODEL DETECTION METRICS ===\n",
      "Precision: 0.0079\n",
      "Recall:    0.0030\n",
      "F1 Score:  0.0044\n",
      "Avg IoU:   0.5516\n",
      "Dice:      0.0044\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# 8. Main Function\n",
    "###########################################\n",
    "def main():\n",
    "    # Parse ground-truth annotations from XML\n",
    "    ground_truth = parse_xml(xml_file)\n",
    "\n",
    "    # Create background subtractor\n",
    "    fgbg = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=40, detectShadows=False)\n",
    "\n",
    "    # Create multi-object tracker instance\n",
    "    tracker = MultiObjectTracker(max_disappeared=10, max_distance=30, alpha=0.8)\n",
    "\n",
    "    # Evaluate the hybrid model\n",
    "    precision, recall, f1, avg_iou, dice = evaluate_hybrid(ground_truth, video_file, fgbg, tracker, iou_threshold=0.5)\n",
    "    \n",
    "    print(\"=== HYBRID MODEL DETECTION METRICS ===\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"Avg IoU:   {avg_iou:.4f}\")\n",
    "    print(f\"Dice:      {dice:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Metrix Calculation 2 : Tracking Performance Evaluation using MOTA and MOTP : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section evaluates multi-object tracking accuracy and precision using two key metrics:\n",
    "\n",
    "- MOTA (Multiple Object Tracking Accuracy): Measures overall tracking performance by accounting for false positives, false negatives, and ID switches.\n",
    "- MOTP (Multiple Object Tracking Precision): Calculates the average Intersection over Union (IoU) to assess how accurately objects are localized.\n",
    "- Hybrid Detection (MOG2 + HOG): Improves object detection by combining motion-based and feature-based methods.\n",
    "- Kalman Filter + Hungarian Algorithm: Tracks objects across frames by predicting motion and optimizing assignments.\n",
    "- Robust Tracking in Crowded Scenes: Enhances pedestrian tracking by integrating motion and appearance-based matching.\n",
    "\n",
    "This evaluation ensures accurate and stable tracking in real-world, dynamic environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# 1. Parse XML for Ground-Truth Tracks\n",
    "###########################################\n",
    "def parse_xml_tracks(xml_file):\n",
    "    \"\"\"\n",
    "    Parses the XML file containing ground-truth tracking annotations.\n",
    "    Each <frame> element has a \"number\" attribute.\n",
    "    Each <object> has an \"id\" and a <box> with attributes: w, h, xc, yc.\n",
    "    Returns a dictionary:\n",
    "       gt_tracks[frame_idx] = { gt_id: (x, y, w, h), ... }\n",
    "    (Coordinates are converted from center-based to top-left-based.)\n",
    "    \"\"\"\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    gt_tracks = {}\n",
    "    for frame_elem in root.findall('frame'):\n",
    "        frame_num = int(frame_elem.get('number'))\n",
    "        tracks = {}\n",
    "        objectlist = frame_elem.find('objectlist')\n",
    "        if objectlist is not None:\n",
    "            for obj in objectlist.findall('object'):\n",
    "                gt_id = int(obj.get('id'))\n",
    "                box_elem = obj.find('box')\n",
    "                if box_elem is not None:\n",
    "                    w = float(box_elem.get('w'))\n",
    "                    h = float(box_elem.get('h'))\n",
    "                    xc = float(box_elem.get('xc'))\n",
    "                    yc = float(box_elem.get('yc'))\n",
    "                    x = xc - w / 2.0\n",
    "                    y = yc - h / 2.0\n",
    "                    tracks[gt_id] = (x, y, w, h)\n",
    "        gt_tracks[frame_num] = tracks\n",
    "    return gt_tracks\n",
    "\n",
    "###########################################\n",
    "# 7. Evaluate Hybrid Tracker (Tracking Metrics)\n",
    "###########################################\n",
    "def evaluate_tracking(gt_tracks, pred_tracks, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluates tracking performance between ground-truth and predicted tracks.\n",
    "    \n",
    "    Parameters:\n",
    "      - gt_tracks: dict { frame_idx: { gt_id: (x,y,w,h), ... } }\n",
    "      - pred_tracks: dict { frame_idx: { pred_id: (x,y,w,h), ... } }\n",
    "      - iou_threshold: IoU threshold for matching\n",
    "      \n",
    "    Returns:\n",
    "      - MOTA: 1 - (FN + FP + ID switches) / total_GT\n",
    "      - MOTP: average IoU over matched detections\n",
    "    \"\"\"\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "    total_idsw = 0\n",
    "    total_gt = 0\n",
    "    iou_sum = 0\n",
    "    total_matches = 0\n",
    "    prev_assignment = {}  # gt_id -> pred_id from previous frame\n",
    "\n",
    "    frames = sorted(gt_tracks.keys())\n",
    "    for frame in frames:\n",
    "        gt_frame = gt_tracks.get(frame, {})     # { gt_id: bbox }\n",
    "        pred_frame = pred_tracks.get(frame, {})   # { pred_id: bbox }\n",
    "        gt_ids = list(gt_frame.keys())\n",
    "        pred_ids = list(pred_frame.keys())\n",
    "        total_gt += len(gt_ids)\n",
    "        \n",
    "        if len(gt_ids) == 0:\n",
    "            total_fp += len(pred_ids)\n",
    "            prev_assignment = {}\n",
    "            continue\n",
    "        \n",
    "        cost_matrix = np.zeros((len(gt_ids), len(pred_ids)), dtype=np.float32)\n",
    "        for i, gt_id in enumerate(gt_ids):\n",
    "            for j, pred_id in enumerate(pred_ids):\n",
    "                iou_val = compute_iou(gt_frame[gt_id], pred_frame[pred_id])\n",
    "                cost_matrix[i, j] = -iou_val  # maximize IoU\n",
    "        \n",
    "        rows, cols = linear_sum_assignment(cost_matrix)\n",
    "        matched_gt = set()\n",
    "        matched_pred = set()\n",
    "        for i, j in zip(rows, cols):\n",
    "            iou_val = -cost_matrix[i, j]\n",
    "            if iou_val >= iou_threshold:\n",
    "                gt_id = gt_ids[i]\n",
    "                pred_id = pred_ids[j]\n",
    "                matched_gt.add(gt_id)\n",
    "                matched_pred.add(pred_id)\n",
    "                iou_sum += iou_val\n",
    "                total_matches += 1\n",
    "                # Check ID switch: if previous frame had a different pred assignment for this gt_id\n",
    "                if gt_id in prev_assignment and prev_assignment[gt_id] != pred_id:\n",
    "                    total_idsw += 1\n",
    "                prev_assignment[gt_id] = pred_id\n",
    "        fp = len(pred_ids) - len(matched_pred)\n",
    "        fn = len(gt_ids) - len(matched_gt)\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "    mota = 1.0 - float(total_fn + total_fp + total_idsw) / float(total_gt) if total_gt > 0 else 0\n",
    "    motp = iou_sum / float(total_matches) if total_matches > 0 else 0\n",
    "    return mota, motp\n",
    "\n",
    "def evaluate_hybrid(ground_truth, video_path, fgbg, tracker, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluates the hybrid model over the video.\n",
    "    Uses the final tracked bounding boxes (from the tracker) as predictions.\n",
    "    :param ground_truth: dict { frame_idx: { gt_id: (x,y,w,h), ... } }\n",
    "    :param video_path: path to the video file\n",
    "    :param fgbg: background subtractor\n",
    "    :param tracker: instance of MultiObjectTracker\n",
    "    :param iou_threshold: IoU threshold for matching\n",
    "    :return: tracking metrics: MOTA, MOTP, and also detection metrics.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_idx = 0\n",
    "    pred_tracks = {}  # frame_idx -> { pred_id: bbox, ... }\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = imutils.resize(frame, width=800)\n",
    "        # Get hybrid detections from background subtraction and HOG\n",
    "        combined_boxes = hybrid_detections(frame, fgbg, conf_thresh=0.5)\n",
    "        # Update tracker with these detections\n",
    "        tracks = tracker.update(frame, combined_boxes)\n",
    "        # Save predicted tracks for this frame (tracker output: {pred_id: data})\n",
    "        frame_preds = {}\n",
    "        for pred_id, data in tracks.items():\n",
    "            frame_preds[pred_id] = data['bbox']\n",
    "        pred_tracks[frame_idx] = frame_preds\n",
    "        frame_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Evaluate tracking: MOTA and MOTP using frame-by-frame matching\n",
    "    mota, motp = evaluate_tracking(ground_truth, pred_tracks, iou_threshold)\n",
    "    return mota, motp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/78/48cyn65d1b1byv818mmyxcx40000gp/T/ipykernel_67375/1527674668.py:31: RuntimeWarning: invalid value encountered in sqrt\n",
      "  overlap = np.sum(np.sqrt(histA * histB))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRACKING METRICS ===\n",
      "MOTA: -0.3775\n",
      "MOTP: 0.5631\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "# 8. Main Function: Putting It All Together\n",
    "###########################################\n",
    "def main():\n",
    "    # Parse ground-truth tracking annotations from XML.\n",
    "    gt_tracks = parse_xml_tracks(xml_file)\n",
    "    \n",
    "    # Create background subtractor (tune parameters as needed)\n",
    "    fgbg = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=40, detectShadows=False)\n",
    "    \n",
    "    # Create multi-object tracker instance\n",
    "    tracker = MultiObjectTracker(max_disappeared=10, max_distance=30, alpha=0.8)\n",
    "    \n",
    "    # Evaluate the hybrid model for tracking\n",
    "    mota, motp = evaluate_hybrid(gt_tracks, video_file, fgbg, tracker, iou_threshold=0.5)\n",
    "    \n",
    "    print(\"=== TRACKING METRICS ===\")\n",
    "    print(f\"MOTA: {mota:.4f}\")\n",
    "    print(f\"MOTP: {motp:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
